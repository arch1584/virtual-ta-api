from transformers import pipeline

qa_pipeline = pipeline("text-generation", model="google/flan-t5-base", max_new_tokens=256)

def generate_answer(question, context_chunks):
    if not question or not isinstance(question, str):
        raise ValueError("Question must be a non-empty string.")

    if not context_chunks or not isinstance(context_chunks, list):
        print("Warning: context_chunks is None or empty. Using fallback context.")
        context_chunks = ["No relevant context available. Try rephrasing your question."]

    context = "\n\n".join(context_chunks)
    prompt = f"Answer the following question based only on the provided context:\n\nContext:\n{context}\n\nQuestion: {question}"

    try:
        result = qa_pipeline(prompt)
        if isinstance(result, list) and len(result) > 0:
            answer = result[0]["generated_text"].strip()
            return answer
        else:
            print("No answer generated by the local model.")
            return "No answer generated. Please try again."
    except Exception as e:
        print(f"Local model error: {e}")
        return "An error occurred while generating an answer."
